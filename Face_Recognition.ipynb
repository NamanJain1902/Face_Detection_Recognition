{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "feAfj-5Gib9J",
    "outputId": "ea05846b-3a15-4d8a-e300-8702ec7e159b"
   },
   "outputs": [],
   "source": [
    "# Feed image for image detection\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascPath = \"./haarcascade_frontalface_default.xml\"\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9oEs4j1ivxk"
   },
   "outputs": [],
   "source": [
    "# path to cropped face images\n",
    "path='./tmp/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8WTJ__FHi4mY"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JbYplJagkWc2"
   },
   "outputs": [],
   "source": [
    "# Get Image names stored in \"Images\" folder\n",
    "image_path_names=[]\n",
    "person_names=set()\n",
    "for folder_name in glob.glob(path + 'train/*'):\n",
    "    person_names.add(folder_name.split('/')[-1])\n",
    "    for file_name in glob.glob(folder_name + '/*'):\n",
    "        image_path_names.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B0VQO0oRkoKe",
    "outputId": "a7cc06b0-96b2-4881-ed39-97062b798663"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_path_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BzJua7rHll-Q",
    "outputId": "e4ee005e-1d28-48e1-9749-d14773e229e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aman', 'ayush', 'naman'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uXjqG2ZnRFTD"
   },
   "outputs": [],
   "source": [
    "os.mkdir(path+'test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ff0ekPxwRwWG"
   },
   "outputs": [],
   "source": [
    "# Create Separate folder for each person in \"Test_Images_crop\" folder\n",
    "for person in person_names:\n",
    "  os.mkdir(path+'/test/'+person+'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rp8xv8XO_JTS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-06 19:09:56.570350: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cert/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-06-06 19:09:56.570459: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import ZeroPadding2D,Convolution2D,MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense,Dropout,Softmax,Flatten,Activation,BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4_j4qVk_Pze"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-06 19:11:35.554505: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/cert/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-06-06 19:11:35.554614: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-06 19:11:35.554693: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (nj): /proc/driver/nvidia/version does not exist\n",
      "2022-06-06 19:11:35.555710: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 19:11:36.249437: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 411041792 exceeds 10% of free system memory.\n",
      "2022-06-06 19:11:36.564440: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 411041792 exceeds 10% of free system memory.\n",
      "2022-06-06 19:11:36.660066: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 411041792 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "#Define VGG_FACE_MODEL architecture\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights_by_name(model, path, verbose=False):\n",
    "    import h5py\n",
    "    def load_model_weights(cmodel, weights):\n",
    "        for layer in cmodel.layers:\n",
    "            print(layer.name)\n",
    "            if hasattr(layer, 'layers'):\n",
    "                load_model_weights(layer, weights[layer.name])\n",
    "            else:\n",
    "                for w in layer.weights:\n",
    "                    _, name = w.name.split('/')\n",
    "                    if verbose:\n",
    "                        print(w.name)\n",
    "                    try:\n",
    "                        w.assign(weights[layer.name][name][()])\n",
    "                    except:\n",
    "                        w.assign(weights[layer.name][layer.name][name][()])\n",
    "\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        load_model_weights(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_weights_by_name(model, './vgg-face-keras.h5', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "u7k9g_wtCUfD",
    "outputId": "3f5b8306-f5ca-44fa-bf1b-ae32b79e4d89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "zero_padding2d (ZeroPadding2 (None, 226, 226, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 226, 226, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 114, 114, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 114, 114, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 58, 58, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 1, 1, 4096)        102764544 \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 1, 4096)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 1, 1, 4096)        16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 1, 4096)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 1, 1, 2622)        10742334  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2622)              0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2622)              0         \n",
      "=================================================================\n",
      "Total params: 145,002,878\n",
      "Trainable params: 145,002,878\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZ21pD4FDLkT"
   },
   "outputs": [],
   "source": [
    "# Remove Last Softmax layer and get model upto last flatten layer with outputs 2622 units\n",
    "vgg_face=Model(inputs=model.layers[0].input,outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zSqFno_Epfz"
   },
   "outputs": [],
   "source": [
    "#Prepare Training Data\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "person_folders=os.listdir(path+'/train/')\n",
    "person_rep=dict()\n",
    "for i,person in enumerate(person_folders):\n",
    "    person_rep[i]=person\n",
    "    image_names=os.listdir(path + 'train/'+person+'/')\n",
    "    for image_name in image_names:\n",
    "        img=load_img(path+'/train/'+person+'/'+image_name,target_size=(224,224))\n",
    "        img=img_to_array(img)\n",
    "        img=np.expand_dims(img,axis=0)\n",
    "        img=preprocess_input(img)\n",
    "        img_encode=vgg_face(img)\n",
    "        x_train.append(np.squeeze(K.eval(img_encode)).tolist())\n",
    "        y_train.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "8Cl8ewCNRRCu",
    "outputId": "95a535f3-b2c9-4c05-ec7a-4633e28478e4",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'ayush', 1: 'aman', 2: 'naman'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QyxgU05lSX4D"
   },
   "outputs": [],
   "source": [
    "x_train=np.array(x_train)\n",
    "y_train=np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aylVlQO2SZXB"
   },
   "outputs": [],
   "source": [
    "#Prepare Test Data\n",
    "x_test=[]\n",
    "y_test=[]\n",
    "person_folders=os.listdir(path+'/test/')\n",
    "for i,person in enumerate(person_folders):\n",
    "    image_names=os.listdir(path+'test/'+person+'/')\n",
    "    for image_name in image_names:\n",
    "        img=load_img(path+'/test/'+person+'/'+image_name,target_size=(224,224))\n",
    "        img=img_to_array(img)\n",
    "        img=np.expand_dims(img,axis=0)\n",
    "        img=preprocess_input(img)\n",
    "        img_encode=vgg_face(img)\n",
    "        x_test.append(np.squeeze(K.eval(img_encode)).tolist())\n",
    "        y_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C4NNw5lqTuKL"
   },
   "outputs": [],
   "source": [
    "x_test=np.array(x_test)\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EAT3ehPVT-24"
   },
   "outputs": [],
   "source": [
    "# Save test and train data for later use\n",
    "np.save('train_data',x_train)\n",
    "np.save('train_labels',y_train)\n",
    "np.save('test_data',x_test)\n",
    "np.save('test_labels',y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sQ5aWACqUnX3"
   },
   "outputs": [],
   "source": [
    "# Load saved data\n",
    "x_train=np.load('train_data.npy')\n",
    "y_train=np.load('train_labels.npy')\n",
    "x_test=np.load('test_data.npy')\n",
    "y_test=np.load('test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wZOEaNOb0qxD"
   },
   "outputs": [],
   "source": [
    "# Softmax regressor to classify images based on encoding \n",
    "classifier_model=Sequential()\n",
    "classifier_model.add(Dense(units=100,input_dim=x_train.shape[1],kernel_initializer='glorot_uniform'))\n",
    "classifier_model.add(BatchNormalization())\n",
    "classifier_model.add(Activation('tanh'))\n",
    "classifier_model.add(Dropout(0.3))\n",
    "classifier_model.add(Dense(units=10,kernel_initializer='glorot_uniform'))\n",
    "classifier_model.add(BatchNormalization())\n",
    "classifier_model.add(Activation('tanh'))\n",
    "classifier_model.add(Dropout(0.2))\n",
    "classifier_model.add(Dense(units=6,kernel_initializer='he_uniform'))\n",
    "classifier_model.add(Activation('softmax'))\n",
    "classifier_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),optimizer='nadam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "fNJsffbD4ulB",
    "outputId": "192feb14-2266-42b2-f26b-db35d637e456"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-06 10:22:44.265760: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11/11 [==============================] - 29s 81ms/step - loss: 0.7705 - accuracy: 0.8192 - val_loss: 1.5891 - val_accuracy: 0.3333\n",
      "Epoch 2/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.4221 - accuracy: 0.9738 - val_loss: 1.5667 - val_accuracy: 0.3333\n",
      "Epoch 3/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.3658 - accuracy: 0.9825 - val_loss: 1.5352 - val_accuracy: 0.3333\n",
      "Epoch 4/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.3293 - accuracy: 0.9796 - val_loss: 1.5155 - val_accuracy: 0.3333\n",
      "Epoch 5/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.2955 - accuracy: 1.0000 - val_loss: 1.4744 - val_accuracy: 0.3333\n",
      "Epoch 6/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.2669 - accuracy: 1.0000 - val_loss: 1.4366 - val_accuracy: 0.4444\n",
      "Epoch 7/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.2454 - accuracy: 0.9971 - val_loss: 1.3715 - val_accuracy: 0.6667\n",
      "Epoch 8/100\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.2407 - accuracy: 0.9942 - val_loss: 1.3255 - val_accuracy: 0.6667\n",
      "Epoch 9/100\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.2523 - accuracy: 0.9913 - val_loss: 1.5219 - val_accuracy: 0.6667\n",
      "Epoch 10/100\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.2078 - accuracy: 0.9971 - val_loss: 1.2156 - val_accuracy: 0.6667\n",
      "Epoch 11/100\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.1672 - accuracy: 1.0000 - val_loss: 1.1984 - val_accuracy: 0.6667\n",
      "Epoch 12/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.1798 - accuracy: 1.0000 - val_loss: 1.1493 - val_accuracy: 0.7778\n",
      "Epoch 13/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.1601 - accuracy: 0.9971 - val_loss: 1.0921 - val_accuracy: 0.8889\n",
      "Epoch 14/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.1522 - accuracy: 1.0000 - val_loss: 1.0315 - val_accuracy: 0.8889\n",
      "Epoch 15/100\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.1256 - accuracy: 1.0000 - val_loss: 1.0536 - val_accuracy: 0.8889\n",
      "Epoch 16/100\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.1440 - accuracy: 1.0000 - val_loss: 0.9580 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.1085 - accuracy: 0.9971 - val_loss: 0.9695 - val_accuracy: 0.8889\n",
      "Epoch 18/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.1152 - accuracy: 1.0000 - val_loss: 0.9527 - val_accuracy: 0.8889\n",
      "Epoch 19/100\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.1166 - accuracy: 1.0000 - val_loss: 0.8671 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.1023 - accuracy: 1.0000 - val_loss: 0.7980 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0956 - accuracy: 1.0000 - val_loss: 0.7383 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0806 - accuracy: 1.0000 - val_loss: 0.7490 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.0857 - accuracy: 1.0000 - val_loss: 0.7514 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0916 - accuracy: 1.0000 - val_loss: 0.6796 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.0802 - accuracy: 1.0000 - val_loss: 0.6096 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.0753 - accuracy: 1.0000 - val_loss: 0.5610 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0706 - accuracy: 1.0000 - val_loss: 0.4969 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0669 - accuracy: 1.0000 - val_loss: 0.4355 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.0639 - accuracy: 1.0000 - val_loss: 0.4371 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0819 - accuracy: 1.0000 - val_loss: 0.3991 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0679 - accuracy: 1.0000 - val_loss: 0.3293 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0580 - accuracy: 1.0000 - val_loss: 0.3577 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0568 - accuracy: 1.0000 - val_loss: 0.3849 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0540 - accuracy: 1.0000 - val_loss: 0.2552 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.0497 - accuracy: 1.0000 - val_loss: 0.2357 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0538 - accuracy: 1.0000 - val_loss: 0.2285 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0465 - accuracy: 1.0000 - val_loss: 0.2054 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0490 - accuracy: 1.0000 - val_loss: 0.2588 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0421 - accuracy: 1.0000 - val_loss: 0.2013 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.0418 - accuracy: 1.0000 - val_loss: 0.1300 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.0448 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.0430 - accuracy: 1.0000 - val_loss: 0.1043 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.1185 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.0346 - accuracy: 1.0000 - val_loss: 0.0849 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0334 - accuracy: 1.0000 - val_loss: 0.0804 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.0600 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.0370 - accuracy: 1.0000 - val_loss: 0.0573 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.0389 - accuracy: 1.0000 - val_loss: 0.1009 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 0.0488 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.0334 - accuracy: 1.0000 - val_loss: 0.0495 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0274 - accuracy: 1.0000 - val_loss: 0.0357 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0327 - accuracy: 1.0000 - val_loss: 0.0268 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.0270 - accuracy: 1.0000 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.0291 - accuracy: 1.0000 - val_loss: 0.0421 - val_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0259 - accuracy: 0.9971 - val_loss: 0.0213 - val_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 0.0195 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0273 - accuracy: 1.0000 - val_loss: 0.0207 - val_accuracy: 1.0000\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 8ms/step - loss: 0.0207 - accuracy: 1.0000 - val_loss: 0.0160 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0241 - accuracy: 1.0000 - val_loss: 0.0209 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 0.1812 - val_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.0424 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 0.0175 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0285 - accuracy: 1.0000 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.0137 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 0.0099 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.0210 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0232 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0188 - accuracy: 1.0000 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.0233 - accuracy: 1.0000 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.0116 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.0206 - accuracy: 1.0000 - val_loss: 0.0073 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 0.0168 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.0323 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.1033 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.0094 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0167 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.0088 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.0084 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.0066 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.0055 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0271 - accuracy: 0.9971 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0074 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.0764 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 0.0223 - accuracy: 0.9971 - val_loss: 0.3674 - val_accuracy: 0.8889\n",
      "Epoch 94/100\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.1933 - val_accuracy: 0.8889\n",
      "Epoch 95/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.0570 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.0378 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.0421 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3ba3c29460>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_model.fit(x_train,y_train,epochs=100,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5BtgAfSo8XW1"
   },
   "outputs": [],
   "source": [
    "# Save model for later use\n",
    "tf.keras.models.save_model(classifier_model,'face_classifier_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3c-QvTzD_Xmf"
   },
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "classifier_model=tf.keras.models.load_model('face_classifier_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AJZg7e93Cgu1"
   },
   "outputs": [],
   "source": [
    "def plot(img):\n",
    "  plt.figure(figsize=(8,4))\n",
    "  plt.imshow(img[:,:,::-1])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wofuBLTOH7Iu"
   },
   "outputs": [],
   "source": [
    "# Label names for class numbers\n",
    "person_rep={0: 'ayush', 1: 'aman', 2: 'naman'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0M57Q7chO_Xc"
   },
   "outputs": [],
   "source": [
    "os.mkdir(path+'/Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(path+'/Test_Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "oJPEhDUbBHUH",
    "outputId": "5447cea7-3810-47af-b620-81c282efaae8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for img_name in os.listdir(path+'Test_Images/'):\n",
    "    print(img_name)\n",
    "    if img_name=='crop_img.jpg':\n",
    "        continue\n",
    "    \n",
    "    # Load Image\n",
    "    img=cv2.imread(path+'/Test_Images/'+img_name)\n",
    "    gray=cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect Faces\n",
    "    faces = faceCascade.detectMultiScale(gray,\n",
    "                                         scaleFactor=1.1,\n",
    "                                         minNeighbors=5,\n",
    "                                         minSize=(60, 60),\n",
    "                                         flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    \n",
    "    left,top,right,bottom=0,0,0,0\n",
    "    for (i, face) in enumerate(faces):\n",
    "        # Extract Each Face\n",
    "        x, y, w, h = face\n",
    "        print(face)\n",
    "        img_crop=img[y:y+h,x:x+w]\n",
    "        cv2.imwrite(path+'/Test_Images/crop_img.jpg',img_crop)\n",
    "\n",
    "        # Get Embeddings\n",
    "        crop_img=load_img(path+'/Test_Images/crop_img.jpg',target_size=(224,224))\n",
    "        crop_img=img_to_array(crop_img)\n",
    "        crop_img=np.expand_dims(crop_img,axis=0)\n",
    "        crop_img=preprocess_input(crop_img)\n",
    "        img_encode=vgg_face(crop_img)\n",
    "\n",
    "        # Make Predictions\n",
    "        embed  = K.eval(img_encode)\n",
    "        person = classifier_model.predict(embed)\n",
    "        name   = person_rep[np.argmax(person)]\n",
    "        os.remove(path + '/Test_Images/crop_img.jpg')\n",
    "        \n",
    "        cv2.rectangle(img,(x, y),(x+w,y+h),(0, 255, 0), 2)\n",
    "        img=cv2.putText(img,name,(x,y),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,255),2,cv2.LINE_AA)\n",
    "        img=cv2.putText(img,str(np.max(person)),(x,y+h),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,0),1,cv2.LINE_AA)\n",
    "      \n",
    "    # Save images with bounding box,name and accuracy \n",
    "    cv2.imwrite(path+'/Predictions/'+img_name,img)\n",
    "    plot(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JzwYUoaAYXkx"
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     success, img = cap.read()\n",
    "\n",
    "#     faces = faceCascade.detectMultiScale(img,\n",
    "#                                          scaleFactor=1.1,\n",
    "#                                          minNeighbors=5,\n",
    "#                                          minSize=(60, 60),\n",
    "#                                          flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    \n",
    "#     left,top,right,bottom=0,0,0,0\n",
    "#     for (i, face) in enumerate(faces):\n",
    "#         # Extract Each Face\n",
    "#         x, y, w, h = face\n",
    "#         print(face)\n",
    "#         img_crop=img[y:y+h,x:x+w]\n",
    "#         cv2.imwrite(path+'/Test_Images/crop_img.jpg',img_crop)\n",
    "\n",
    "#         # Get Embeddings\n",
    "#         crop_img=load_img(path+'/Test_Images/crop_img.jpg',target_size=(224,224))\n",
    "#         crop_img=img_to_array(crop_img)\n",
    "#         crop_img=np.expand_dims(crop_img,axis=0)\n",
    "#         crop_img=preprocess_input(crop_img)\n",
    "#         img_encode=vgg_face(crop_img)\n",
    "\n",
    "#         # Make Predictions\n",
    "#         embed  = K.eval(img_encode)\n",
    "#         person = classifier_model.predict(embed)\n",
    "#         name   = person_rep[np.argmax(person)]\n",
    "#         os.remove(path + '/Test_Images/crop_img.jpg')\n",
    "        \n",
    "#         cv2.rectangle(img,(x, y),(x+w,y+h),(0, 255, 0), 2)\n",
    "#         img=cv2.putText(img,name,(x,y),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,255),2,cv2.LINE_AA)\n",
    "#         img=cv2.putText(img,str(np.max(person)),(x,y+h),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,0),1,cv2.LINE_AA)\n",
    "      \n",
    "#     # Save images with bounding box,name and accuracy \n",
    "#     if img is not None:\n",
    "#         cv2.imshow(\"Frame\", img)\n",
    "    \n",
    "#     if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Face_Recognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
